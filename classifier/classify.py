"""
This script does action classification and extract keywords from the incoming query.

Author: Wenjun Wang
Date: June 29, 2015

TODO(wenjunw@cs.cmu.edu):
- Reconsider the type words
"""
import pickle
import datetime
import nltk

from feature import *
from liblinearutil import *

class Classifier(object):
    def __init__(self):
        """
        All variables which would be used by every query classification and parsing are listed here.
        Only need to create Classifier object once, i.e. initialize once
        """    
        self.model = self._get_model()
        self.stopwords = stopword('english.stp')
        self.feature_arg = parse_options('-uni -pos2')
        self.feature_list = self._get_feature_list()
        self.type_words = self._set_type_words()
        self.labels = [1,2,3,4,5,6,7]

    def _get_model(self):
        """Load model

        This function is called during initialization

        Return: model
        """
        date = str(datetime.date.today())
        m = load_model('models/model_'+date)
        if m == None:
            date = str(datetime.date.fromordinal(datetime.date.today().toordinal()-1))
            m = load_model('models/model_'+date)

        return m

    def _get_feature_list(self):
        """Load feature file

        This function is called during initialization

        Return: Feature list
        """
        date = str(datetime.date.today())
        try:
            infile = open('models/features_'+date)
        except IOError:
            date = str(datetime.date.fromordinal(datetime.date.today().toordinal()-1))
            infile = open('models/features_'+date)

        feature_list = pickle.load(infile)
        return feature_list

    def _convert_query_to_dictionary(self, query):
        """Convert each user query to the format required by LibLINEAR

        This function is called by self._classify(query)

        Args and Need: 
            query: the raw query, like 'What do people think of ?'
            self.feature_list: a list of unique features generated by function feature_generator
    
        Return:
            Convert user's query: store information in a dictionary, 
            which is a member of a list. 
        """
        features = feature_generator(query, self.stopwords, self.feature_arg)
        onerow = {}
        for f in features:
            try:
                onerow[self.feature_list.index(f)+1] = 1
            except ValueError:
                pass

        return [onerow]

    def _classify(self, query):
        """Does query classification, which decides which action need to be taken

        This function is called by self.action_info

        Return: Action id
        """
        x = self._convert_query_to_dictionary(query)
        p_label, p_val = predict(self.labels, x, self.model, '-b 0')
        #print p_val
        if p_val[0][int(p_label[0])-1] == 0:
            p_label[0] = -1

        return int(p_label[0])

    def action_info(self, query, plausible):
        """API function in this script. Gives all info of an action

        This is the only function which will be called outside this script.

        Args:
            query: query need to classify and parse
            plausible: a set of plausible actions at current step

        Return:
            arguments: a dictionary contains all the info needed by calling function

        """
        arguments = {}
        if 8 in plausible:
            self._type_recognition(query, arguments)
        elif 7 in plausible and len(plausible) == 1:
            self._entity_recognition(query,arguments)
        else:
            arguments['aid'] = self._classify(query)
            if arguments['aid'] == 7:
                self._entity_recognition(query,arguments)

        return arguments

    def _entity_recognition(self, query, arguments):
        """Parse query and extract keywords

        This function is called by self.action_info

        Args:
            query: query needs to be parsed
            arguments: info needs to be updated
        """
        tokens = nltk.word_tokenize(query)
        tags = nltk.pos_tag(tokens)
        entities = nltk.chunk.ne_chunk(tags)
        arguments['aid'] = 7
        #print entities

        tuples = []
        trees = []
        for i in entities:
            if isinstance(i,tuple):
                if ((i[1][:2] == 'NN' or i[1][:2] == 'JJ')
                    and i[0].lower() not in self.stopwords 
                    and i[0].rstrip('s') not in self.type_words['movie']
                    and i[0].rstrip('s') not in self.type_words['article'] 
                    and i[0].rstrip('s') not in self.type_words['restaurant']):
                    tuples.append(i[0])
            elif isinstance(i,nltk.tree.Tree):
                phrase = []
                for element in i:
                    if element[0].lower() not in self.stopwords:
                        phrase.append(element[0])
                if len(phrase) > 0:
                    trees.append(' '.join(phrase))

        if len(trees) > 0:
            arguments['keywords'] = '#'.join(trees).strip('#')
        elif len(tuples) > 0:
            arguments['keywords'] = '#'.join(tuples).strip('#')
        else:
            arguments['aid'] = -1
    
    def _set_type_words(self):
        """Initialize synonymy words of movie, article and restaurant

        This function is called during initialization

        Return: A dictionary, key: movie, article, restaurant, value: their synonymy words
        """
        topic = {}
        topic['movie'] = set(['cinema','show','film','picture','cinematograph',
            'videotape','flick','pic','cine','cinematics','photodrama',
            'photoplay','talkie','flicker','DVD','movie'])
        topic['article'] = set(['report','announcement','story','account',
            'newscast','headlines','press','communication','talk','word',
            'communique','bulletin','message','dispatch','broadcast',
            'statement','intelligence','disclosure','revelation',
            'gossip','dispatch','news','article'])
        topic['restaurant'] = set(['bar','cafeteria','diner','dining','saloon','coffeehouse',
            'canteen','chophouse','drive-in','eatery','grill','lunchroom','inn',
            'pizzeria','hideaway','cafe','charcuterie','deli','restaurant'])
        return topic

    def _type_recognition(self, query, arguments):
        """Identity the type of the topic: movie, article or restaurant

        This is called by self.action_info

        Args:
            query: query needs to be parsed
            arguments: info needs to be updated

        """
        tokens = nltk.word_tokenize(query)
        arguments['aid'] = 8
        if len(tokens) > 1:
            if tokens[-1].rstrip('s') in self.type_words['article'] or tokens[-2].rstrip('s') in self.type_words['article']:
                arguments['tid'] = 1
            elif tokens[-1].rstrip('s') in self.type_words['restaurant'] or tokens[-2].rstrip('s') in self.type_words['restaurant']:
                arguments['tid'] = 2
            elif tokens[-1].rstrip('s') in self.type_words['movie'] or tokens[-2].rstrip('s') in self.type_words['movie']:
                arguments['tid'] = 3
            else:
                arguments['aid'] = -1
        else:
            if tokens[-1].rstrip('s') in self.type_words['article']:
                arguments['tid'] = 1
            elif tokens[-1].rstrip('s') in self.type_words['restaurant']:
                arguments['tid'] = 2
            elif tokens[-1].rstrip('s') in self.type_words['movie']:
                arguments['tid'] = 3
            else:
                arguments['aid'] = -1